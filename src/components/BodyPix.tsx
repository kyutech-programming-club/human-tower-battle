import { useRef, useEffect, useState, useCallback } from "react";
import * as tf from "@tensorflow/tfjs";
import * as bodyPix from "@tensorflow-models/body-pix";
import { saveCanvasToIndexedDB } from "../utils/db";

// =======================================
// å®šæ•°
// =======================================
const SEGMENTATION_CONFIG = {
  OPACITY: 1.0,
  BLUR_AMOUNT: 2.0,
  FLIP_HORIZONTAL: false,
} as const;

const CANVAS_STYLE = {
  width: 450,
  height: 300,
} as const;

// =======================================
// ã‚°ãƒªãƒ¼ãƒ³æ¤œå‡º & å…±é€šå‡¦ç†ï¼ˆè¿½åŠ ï¼‰
// =======================================

// RGB(0..255) â†’ HSV(åº¦/0..1/0..1)
const rgbToHsv = (r: number, g: number, b: number) => {
  const rf = r / 255, gf = g / 255, bf = b / 255;
  const max = Math.max(rf, gf, bf), min = Math.min(rf, gf, bf);
  const d = max - min;
  let h = 0;
  if (d !== 0) {
    switch (max) {
      case rf: h = ((gf - bf) / d) % 6; break;
      case gf: h = (bf - rf) / d + 2; break;
      case bf: h = (rf - gf) / d + 4; break;
    }
    h *= 60;
    if (h < 0) h += 360;
  }
  const s = max === 0 ? 0 : d / max;
  const v = max;
  return { h, s, v };
};

// ã‚¯ãƒ­ãƒã‚­ãƒ¼ï¼ˆç·‘ï¼‰ã¨ã¿ãªã™ç¯„å›²ï¼šç¾å ´ã«å¿œã˜ã¦èª¿æ•´å¯
const GREEN_KEY = {
  H_MIN: 70,   // é»„ç·‘å¯„ã‚Šãªã‚‰ 60 ã«ä¸‹ã’ã‚‹
  H_MAX: 160,  // é’ç·‘å¯„ã‚Šãªã‚‰ 170 ã¸ä¸Šã’ã‚‹
  S_MIN: 0.25, // å½©åº¦ã—ãã„
  V_MIN: 0.20, // æ˜åº¦ã—ãã„
};

const isGreenPixel = (r: number, g: number, b: number) => {
  const { h, s, v } = rgbToHsv(r, g, b);
  return (
    h >= GREEN_KEY.H_MIN && h <= GREEN_KEY.H_MAX &&
    s >= GREEN_KEY.S_MIN && v >= GREEN_KEY.V_MIN
  );
};

// äººç‰©ãƒã‚¹ã‚¯ï¼ˆ0/1ï¼‰ã‹ã‚‰ã€Œç·‘ã£ã½ã„éƒ¨åˆ†ã€ã‚’é™¤ã„ãŸè£œæ­£ãƒã‚¹ã‚¯ã‚’è¿”ã™
const refineMaskWithGreen = (
  personMask: Uint8Array | Int32Array | number[],
  imageData: ImageData
) => {
  const data = imageData.data; // RGBA
  const n = personMask.length;
  const refined = new Uint8Array(n);
  for (let i = 0; i < n; i++) {
    if (personMask[i] === 1) {
      const idx = i * 4;
      const r = data[idx], g = data[idx + 1], b = data[idx + 2];
      refined[i] = isGreenPixel(r, g, b) ? 0 : 1;
    } else {
      refined[i] = 0;
    }
  }
  return refined;
};

// BodyPixã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨è«–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
const DEFAULT_SEGMENT_OPTS = {
  flipHorizontal: SEGMENTATION_CONFIG.FLIP_HORIZONTAL,
  internalResolution: "medium" as const,
  segmentationThreshold: 0.7,
};

/**
 * å…±é€šå‡¦ç†ï¼š
 * 1) segmentPerson
 * 2) (å¿…è¦ãªã‚‰åè»¢ã—ã¦) video ã‚’ canvas ã«æç”»
 * 3) imageData ã‚’å–å¾—
 * 4) ç·‘é™¤å»è£œæ­£ãƒã‚¹ã‚¯ç”Ÿæˆ
 * 5) Î±ã‚’æ›¸ãæ›ãˆï¼ˆäººç‰©=255, èƒŒæ™¯=0ï¼‰
 * 6) putImageData
 */
const segmentDrawAndApplyAlpha = async (params: {
  video: HTMLVideoElement;
  canvas: HTMLCanvasElement;
  model: bodyPix.BodyPix;
  segmentOpts?: Partial<typeof DEFAULT_SEGMENT_OPTS>;
}) => {
  const { video, canvas, model, segmentOpts } = params;
  const opts = { ...DEFAULT_SEGMENT_OPTS, ...(segmentOpts ?? {}) };

  const ctx = canvas.getContext("2d");
  if (!ctx) throw new Error("2D context not available");

  // ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚µã‚¤ã‚ºåŒæœŸ
  if (canvas.width !== video.videoWidth || canvas.height !== video.videoHeight) {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
  }

  // 1) ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
  const segmentation = await model.segmentPerson(video, opts);

  // 2) ãƒ•ãƒ¬ãƒ¼ãƒ æç”»ï¼ˆå¿…è¦ãªã‚‰å·¦å³åè»¢ï¼‰
  ctx.save();
  if (opts.flipHorizontal) {
    ctx.translate(canvas.width, 0);
    ctx.scale(-1, 1);
  }
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
  ctx.restore();

  // 3) ãƒ”ã‚¯ã‚»ãƒ«å–å¾—
  const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

  // 4) ç·‘é™¤å»è£œæ­£ãƒã‚¹ã‚¯
  const refinedMask = refineMaskWithGreen(segmentation.data, imageData);

  // 5) Î±ã‚’æ›¸ãæ›ãˆï¼ˆäººç‰©=255 / èƒŒæ™¯=0ï¼‰
  const data = imageData.data;
  for (let i = 0; i < refinedMask.length; i++) {
    data[i * 4 + 3] = refinedMask[i] ? 255 : 0;
  }

  // 6) æç”»æ›´æ–°
  ctx.putImageData(imageData, 0, 0);

  return { refinedMask, imageData, ctx };
};

// =======================================
// ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯: BodyPixãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
// =======================================
const useBodyPixModel = () => {
  const [model, setModel] = useState<bodyPix.BodyPix | null>(null);
  const [isLoading, setIsLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);

  useEffect(() => {
    const initializeModel = async () => {
      try {
        await tf.setBackend("webgl");
        await tf.ready();
        const net = await bodyPix.load();
        setModel(net);
      } catch (err) {
        setError(
          err instanceof Error ? err.message : "ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ"
        );
      } finally {
        setIsLoading(false);
      }
    };

    initializeModel();
  }, []);

  return { model, isLoading, error };
};

// =======================================
// ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯: ã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®åˆæœŸåŒ–
// =======================================
const useVideoStream = (videoRef: React.RefObject<HTMLVideoElement | null>) => {
  const [isVideoReady, setIsVideoReady] = useState(false);
  const [error, setError] = useState<string | null>(null);

  useEffect(() => {
    let isMounted = true;

    const initializeVideo = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: true,
          audio: false,
        });

        if (!isMounted) return;

        if (videoRef.current) {
          const video = videoRef.current;

          // æ—¢å­˜ã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒã‚ã‚‹å ´åˆã¯åœæ­¢
          if (video.srcObject) {
            const existingStream = video.srcObject as MediaStream;
            existingStream.getTracks().forEach((track) => track.stop());
          }

          video.srcObject = stream;

          const handleLoadedMetadata = async () => {
            if (!isMounted) return;
            try {
              await video.play();
              if (isMounted) setIsVideoReady(true);
            } catch (playError) {
              if (isMounted) {
                setError(
                  playError instanceof Error
                    ? playError.message
                    : "ãƒ“ãƒ‡ã‚ªã®å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ"
                );
              }
            }
          };

          if (video.readyState >= 1) {
            handleLoadedMetadata();
          } else {
            video.addEventListener("loadedmetadata", handleLoadedMetadata, {
              once: true,
            });
          }
        }
      } catch (err) {
        if (isMounted) {
          setError(
            err instanceof Error ? err.message : "ã‚«ãƒ¡ãƒ©ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ"
          );
        }
      }
    };

    initializeVideo();

    return () => {
      isMounted = false;
      if (videoRef.current?.srcObject) {
        const stream = videoRef.current.srcObject as MediaStream;
        stream.getTracks().forEach((track) => track.stop());
        videoRef.current.srcObject = null;
      }
    };
  }, [videoRef]);

  return { isVideoReady, error };
};

// =======================================
// ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæœ¬ä½“
// =======================================
const BodyPix: React.FC = () => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const rafIdRef = useRef<number | undefined>(undefined);
  const [autoSave, setAutoSave] = useState(false);

  const {
    model,
    isLoading: isModelLoading,
    error: modelError,
  } = useBodyPixModel();
  const { isVideoReady, error: videoError } = useVideoStream(videoRef);

  // ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ©ã‚¤ãƒ–è¡¨ç¤ºï¼‰
  const runSegmentation = useCallback(async () => {
    if (!model || !videoRef.current || !canvasRef.current) return;
    try {
      await segmentDrawAndApplyAlpha({
        video: videoRef.current,
        canvas: canvasRef.current,
        model,
        // segmentOpts: { segmentationThreshold: 0.75 }, // å¿…è¦ãªã‚‰ä¸Šæ›¸ã
      });
    } catch (error) {
      console.error("ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼:", error);
    }
  }, [model]);

  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ«ãƒ¼ãƒ—
  useEffect(() => {
    if (!model || !isVideoReady) return;

    const loop = () => {
      runSegmentation();
      rafIdRef.current = requestAnimationFrame(loop);
    };

    loop();

    return () => {
      if (rafIdRef.current) cancelAnimationFrame(rafIdRef.current);
    };
  }, [model, isVideoReady, runSegmentation]);

  // IndexedDBã«é€éPNGã‚’ä¿å­˜
  const handleSaveToIndexedDB = useCallback(async () => {
    if (!model || !videoRef.current) {
      console.warn("ä¿å­˜ã«å¿…è¦ãªè¦ç´ ãŒæº–å‚™ã§ãã¦ã„ã¾ã›ã‚“ã€‚");
      return;
    }

    try {
      const video = videoRef.current;

      // ä¿å­˜ã¯ä¸€æ™‚ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚’ä½¿ç”¨ï¼ˆUIã‚­ãƒ£ãƒ³ãƒã‚¹ã«ã¯è§¦ã‚Œãªã„ï¼‰
      const tempCanvas = document.createElement("canvas");
      tempCanvas.width = video.videoWidth;
      tempCanvas.height = video.videoHeight;

      await segmentDrawAndApplyAlpha({
        video,
        canvas: tempCanvas,
        model,
      });

      const id = await saveCanvasToIndexedDB(tempCanvas);
      console.log(`ç”»åƒãŒIndexedDBã«ä¿å­˜ã•ã‚Œã¾ã—ãŸï¼ï¼ˆID: ${id}ï¼‰`);
      return id;
    } catch (error) {
      console.error("IndexedDBã¸ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼:", error);
      throw error;
    }
  }, [model]);

  // ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
  const getStatusText = () => {
    if (modelError || videoError) return `ã‚¨ãƒ©ãƒ¼: ${modelError || videoError}`;
    if (isModelLoading) return "ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...";
    if (!isVideoReady) return "ã‚«ãƒ¡ãƒ©åˆæœŸåŒ–ä¸­...";
    return "æº–å‚™å®Œäº†";
  };

  const isReady = model && isVideoReady && !modelError && !videoError;

  // 5ç§’é–“éš”ã§è‡ªå‹•ä¿å­˜
  useEffect(() => {
    if (!autoSave || !isReady) return;

    const interval = setInterval(async () => {
      try {
        await handleSaveToIndexedDB();
      } catch (error) {
        console.error("BodyPixè‡ªå‹•ä¿å­˜ã‚¨ãƒ©ãƒ¼:", error);
      }
    }, 5000);

    return () => clearInterval(interval);
  }, [autoSave, isReady, handleSaveToIndexedDB]);

  return (
    <>
      <div>
        {/* éè¡¨ç¤ºã®ãƒ“ãƒ‡ã‚ªè¦ç´ ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼‰ */}
        <video
          ref={videoRef}
          style={{ position: "absolute", left: "-9999px" }}
          muted
          playsInline
        />

        {/* ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœè¡¨ç¤º */}
        <div style={{ ...CANVAS_STYLE, display: "inline-block" }}>
          <canvas ref={canvasRef} style={CANVAS_STYLE} />
        </div>

        <p>çŠ¶æ…‹: {getStatusText()}</p>
      </div>

      {/* UI */}
      <div
        style={{
          display: "flex",
          gap: "8px",
          marginTop: "8px",
          flexDirection: "column",
        }}
      >
        <button
          onClick={handleSaveToIndexedDB}
          disabled={!isReady}
          style={{
            padding: "8px 16px",
            fontSize: "14px",
            backgroundColor: "#2196F3",
            color: "white",
            border: "none",
            borderRadius: "4px",
          }}
        >
          æ‰‹å‹•ä¿å­˜
        </button>

        <label
          style={{
            display: "flex",
            alignItems: "center",
            gap: "8px",
            fontSize: "14px",
            cursor: isReady ? "pointer" : "not-allowed",
          }}
        >
          <input
            type="checkbox"
            checked={autoSave}
            onChange={(e) => setAutoSave(e.target.checked)}
            disabled={!isReady}
            style={{ cursor: isReady ? "pointer" : "not-allowed" }}
          />
          <span style={{ color: isReady ? "black" : "#999" }}>
            è‡ªå‹•ä¿å­˜ï¼ˆ5ç§’é–“éš”ï¼‰
            {autoSave && isReady ? " ğŸ”„" : ""}
          </span>
        </label>

        <div
          style={{
            fontSize: "12px",
            textAlign: "center",
            color: autoSave && isReady ? "green" : "#666",
            fontWeight: autoSave && isReady ? "bold" : "normal",
          }}
        >
          {autoSave && isReady ? "è‡ªå‹•ä¿å­˜å®Ÿè¡Œä¸­" : "æ‰‹å‹•ä¿å­˜ãƒ¢ãƒ¼ãƒ‰"}
        </div>
      </div>
    </>
  );
};

export default BodyPix;
